# Copyright 2016-2022 Kevin Murray/Gekkonid Consulting
#
# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at http://mozilla.org/MPL/2.0/.

#######################################################################
#                         Helper python code                          #
#######################################################################
#
# Traditionally, this lived in `utils/snkmk.py` or in `import acanthophis`, but
# because of the way snakemake sends execution context to the k8s controller,
# many local files aren't copied. This include any `$module.py` files lying
# alongside and `import $module`'d in rules files. So, I have copied the
# relevant bits from both Norman's original pipeline and Acanthopis to
# `base.rules`, which is sourced first in the Snakefile. This means that
# `import $module` (i.e. `import acanthophis` or `import snkmk`) is now
# `include: "rules/base.rules"`, and you don't use the module name to access
# any function (i.e. it's like `from $module import *`). Obviously don't use
# any of these functions before doing `include: "rules/base.rules"` or the
# functions won't be defined. 

import csv
from collections import defaultdict
from copy import deepcopy
from glob import glob
from os.path import basename, splitext
import os
from sys import stderr
from math import log, inf
from functools import partial

try:
    from ._version import version
    __version__ = version
except ImportError:
    pass

def populate_metadata(config, runlib2samp=None, sample_meta=None, setfile_glob=None):
    try:
        if runlib2samp is None:
            runlib2samp = config["data_paths"]["metadata"]["runlib2samp_file"]
        if setfile_glob is None:
            setfile_glob = config["data_paths"]["metadata"]["setfile_glob"]
    except KeyError as exc:
        raise ValueError("ERROR: metadata files must be configured in config, or passed to populate_metadata()")

    rl2s_meta = parse_metadata(runlib2samp)
    meta_by_rl = {}
    for rl in rl2s_meta:
        meta_by_rl[(rl["run"], rl["library"])] = rl

    RL2S, S2RL = make_runlib2samp(rl2s_meta)
    config["RUNLIB2SAMP"] = RL2S
    config["SAMP2RUNLIB"] = S2RL
    config["SAMPLESETS"] = make_samplesets(config, runlib2samp, setfile_glob)
    config["RAWDATA_PATHS"] = {}
    for rl, meta in meta_by_rl.items():
        r1 = meta.get("read1_uri", "")
        r2 = meta.get("read2_uri", "")
        il = meta.get("interleaved_uri", "")
        rs = meta.get("single_uri", "")
        ret = {}
        if r1 and r2:
            ret["r12"] = {"r1": r1, "r2": r2}
        if r1 and not r2 or r2 and not r1:
            raise ValueError(f"R1 path given without R2 or vice versa for Run/Library {rl}")
        if il:
            ret["il"] = il
        if rs:
            ret["se"] = rs
        if ret == {}:
            raise ValueError(f"No data paths given for Run/Library {rl}")
        config["RAWDATA_PATHS"][rl] = ret


    # In the rl2s we have a column for "qc_type". in the config file, we
    # have a set of adaptorremoval (and potentially other settings) keyed
    # by qc_type, with a sensible default as __default__. So here, we map
    # each runlib to the qc type, which we later query for a given tool in the
    # rules file (in a params: block).
    config["RL_QCTYPE"]= {
        rl: kv.get("qc_type", "__default__")
        for rl, kv in meta_by_rl.items()
    }
    if config["data_paths"].get("references"):
        config["CHROMS"] = make_chroms(config["data_paths"]["references"])
        config["VARCALL_REGIONS"] = {
            vc: make_regions(config["data_paths"]["references"], window=chunks)
            for vc, chunks in config["tool_settings"].get("varcall", {}).get("chunksize", {}).items()
        } 


def parsefai(fai):
    with open(fai) as fh:
        for l in fh:
            cname, clen, _, _, _ = l.split()
            clen = int(clen)
            yield cname, clen


def make_regions(rdict, window=1e6, base=1):
    window = int(window)
    ret = {}
    for refname, refbits in rdict.items():
        fai = refbits['fasta']+".fai"
        windows = []
        curwin = []
        curwinlen = 0
        for cname, clen in parsefai(fai):
            for start in range(0, clen, window):
                wlen = min(clen - start, window)
                windows.append("{}:{:09d}-{:09d}".format(cname, start + base, start+wlen))
        ret[refname] = windows
    return ret


def make_chroms(rdict):
    ret = {}
    for refname, refbits in rdict.items():
        fai = refbits['fasta']+".fai"
        ref = dict()
        for cname, clen in parsefai(fai):
            ref[cname] = clen
        ret[refname] = ref
    return ret


def parse_metadata(s2rl_file):
    meta = []
    with open(s2rl_file) as fh:
        dialect = "excel"
        if s2rl_file.endswith(".tsv"):
            dialect = "excel-tab"
        for run in csv.DictReader(fh, dialect=dialect):
            if not run["library"] or run["library"].lower().startswith("blank"):
                # Skip blanks
                continue
            if run.get("include", "Y").upper() != "Y":
                # Remove non-sequenced ones
                continue
            meta.append({k.lower(): v for k, v in run.items()})
    return meta


def make_runlib2samp(rl2s_meta):
    rl2s = {}
    s2rl = defaultdict(list)
    for run in rl2s_meta:
        rl = (run["run"], run["library"])
        samp = run["sample"]
        rl2s[rl] = samp
        s2rl[samp].append(rl)
    return dict(rl2s), dict(s2rl)


def stripext(path, exts=".txt"):
    if isinstance(exts, str):
        exts = [exts,]
    for ext in exts:
        if path.endswith(ext):
            path = path[:-len(ext)]
    return path


def make_samplesets(config, s2rl_file, setfile_glob):
    ssets = defaultdict(list)
    everything = set(config.get("SAMP2RUNLIB", {}).keys())
    for setfile in glob(setfile_glob):
        setname = stripext(basename(setfile), ".txt")
        with open(setfile) as fh:
            samples = [x.strip() for x in fh]
        for samp in samples:
            if samp not in everything:
                raise ValueError(f"ERROR: sample '{samp}' not in runlib2samp.tsv")
        ssets[setname] = samples
    ssets["all_samples"] = list(sorted(everything))

    if not os.path.exists("data/samplelists"):
        os.makedirs("data/samplelists", exist_ok=True)
    with open("data/samplelists/GENERATED_FILES_DO_NOT_EDIT", "w") as fh:
        print("you're probably looking for", setfile_glob, file=fh)
    for setname, setsamps in ssets.items():
        fname = "data/samplelists/{}.txt".format(setname)
        try:
            with open(fname) as fh:
                currsamps = set([l.strip() for l in fh])
        except IOError:
            currsamps = set()
        if set(setsamps) != currsamps:
            with open(fname, "w") as fh:
                print("WARNING: updating sample sets, this will trigger reruns", setname, file=stderr)
                for s in sorted(setsamps):
                    print(s, file=fh)
    # Don't sort these sample lists
    return ssets


def rule_resources(config, rule, **defaults):
    def resource(wildcards, attempt, value, maxvalue):
        return int(min(value * 2^(attempt-1), maxvalue))
    C = config.get("resources", {})
    maxes = C.get("__max__", {})
    global_defaults = C.get("__default__", {})
    rule_config = C.get(rule, {})
    
    values = {}
    values.update(global_defaults)
    values.update(defaults)
    values.update(rule_config)
    ret = {}

    if C.get("__DEBUG__", False):
        print(f"{rule}:")
    for res, val in values.items():
        maxval = maxes.get(res, inf)
        if val >= maxval:
            val = maxval
        if res == "mem_gb":
            res = "mem_mb"
            val = int(val) * 1024
        if res == "disk_gb":
            res = "disk_mb"
            val = int(val) * 1024
        maxval = maxes.get(res, inf)
        if val >= maxval:
            val = maxval
        # Also broken TODO
        # if isinstance(val, str):
	#    # the logic below allows restarting with increased resources. If
	#    # the resource's value is string, you can't double it with each
	#    # attempt, so just return it as a constant.
	#    # this is used for things like cluster queues etc.
	#    ret[res] = val
	#    if C.get("__DEBUG__", False):
	#        print(f"  {res}: {val}")
	#    continue
        if C.get("__DEBUG__", False):
            print(f"  {res}: {val}  	# Max val = {maxval}")
        ret[res] = val # TODO this is broken: partial(resource, value=val, maxvalue=maxval)
    return ret


def postprocess_rules(workflow, config):
    R = config.get("resources", {})
    D = R.get("__default__", {
        "cores": 1,
        "disk_mb": 1000,
        "mem_mb": 1000,
        "time_min": 120,
        "localrule": 0,
    })
    for rule_obj in workflow.rules:
        n = rule_obj.name

        res = deepcopy(D)
        res.update(rule_obj.resources)
        res.update(R.get(n, {}))
        for key, val in res.items():
            # Blacklist these
            if key not in ["localrule", "cores"]:
                rule_obj.resources[key] = val
        if "cores" in res:
            res["cores"] = int(res["cores"]) # TODO this is broken: float(res['cores']) * float(R.get("__fractional_cpu_scalar__", 1))
            rule_obj.resources["_cores"] = res["cores"]
        if res.get("localrule", 0):
            workflow._localrules.add(n)


shell.prefix = "set -euo pipefail; "

wildcard_constraints:
    run="[^/]+",
    lib="[^/]+",
    aligner="[^/]+",
    sample="[^/]+",
    ref="[^/]+",
    type="[^/]+",

def T(paths):
    if isinstance(paths, str):
        paths = [paths, ]
    opaths = []
    for path in paths:
        opaths.append(config["data_paths"].get("temp_prefix", "") + path)
    return opaths

def L(path):
    return config["data_paths"].get("log_prefix", "") + path

def R(paths, keep_local=False):
    from snakemake.remote import AUTO
    from snakemake.common import parse_uri
    if isinstance(paths, str):
        paths = [paths, ]
    opaths = []
    for path in paths:
        uri = parse_uri(path)
        PM = AUTO.protocol_mapping
        if uri.scheme in AUTO.protocol_mapping:
            try:
                opaths.append(AUTO.remote(path, keep_local=keep_local))
            except:
                opaths.append(AUTO.remote(path))
        else:
            opaths.append(path)
    if len(opaths) == 1:
        return opaths[0]
    return opaths

def P(paths, keep_local=False):
    if isinstance(paths, str):
        paths = [paths, ]
    return [R(config["data_paths"].get("persistent_prefix", "") + p, keep_local=keep_local) for p in paths]
