#######################################################################
#                             Data Paths                              #
#######################################################################
data_paths:
  # Metadata locations.
  metadata:
    # This file maps the FASTQ files associated with a run of a library to a
    # samplename, and to any other relevant metadata.
    runlib2samp_file: "rawdata/rl2s.tsv"
    # This is a shell glob pattern defining the text files containing lists of
    # samplenames per sample set.
    setfile_glob: "rawdata/samplesets/*.txt"
  references:
    lambda:
      fasta: "rawdata/reference/genome.fa"

  # Taxon profiling databases
  kraken:
    Viral: "rawdata/kraken/Viral"
  kaiju:
    Viral:
      nodes: "rawdata/kaiju/Viral/nodes.dmp"
      fmi: "rawdata/kaiju/Viral/kaiju_db_viruses.fmi"
  centrifuge:
    lambda: "rawdata/centrifuge/lambda/lambda.1.cf"
  temp_prefix: "tmp/"
  persistent_prefix: "output/"

#######################################################################
#                      Sample Set Configuration                       #
#######################################################################
#
# This section is where we tell snakemake which files to generate for each set of samples.
# Samplesets are configured as files of sample names (see setfile_glob above). 

samplesets:

  all_samples:
    
    # Alignment of reads to a reference
    align:
      
      # Which aligners should be used to map reads?
      aligners:
        - bwa
        - ngm
      
      # Against which references should we align?
      references:
        # Remember, each reference here must be defined in data_paths/references above.
        - lambda

      # Should we extract unmapped reads from each BAM?
      unmapped_reads: true
      # Should BAMs be kept longer than needed?
      keep_bams: false 
      stats: true
      qualimap: true

    # Taxonomic classification
    kraken:
      dbs:
        - Viral
      # Output (un)-classified read fastqs from Kraken?
      reads: false
    kaiju:
      dbs:
        - Viral
    centrifuge:
      dbs: []
        # Centrifuge is disabled in this example as we use this in CI testing
        # and centrifuge uses too much RAM, causing our tests to fail. To run
        # centrifuge, supply an array/list of databases here as one does above
        # for Kraken & Kaiju, for exampe:
        #
        # - lambda
    
    # Make a sample_R1/R2.fastq.gz file per sample?
    persample_reads: false

    # Run FastQC per input run/library?
    fastqc: true

    # Calculate distances with Mash/KWIP?
    mash: true
    kwip: true

    # Variant calling
    varcall:
      # Organism's ploidy?
      ploidy: 2
      # Prior on the proportion of variable sites (Î˜)
      theta_prior: 0.01

      # Which variant callers to use?
      callers:
        - mpileup
        - freebayes
      # Which short read aligners to use for variant calling? (can be
      # more/less/different to the align section above)
      aligners:
        - bwa

      # Which references to use for variant calling? (again can be
      # more/less/different to the references used for read alignment based
      # analyses above).
      refs:
        - lambda

      # Which set of filter expressions to use? (see tool_settings section below)
      filters:
        - default

      # Use SNPEff to annotate variant effects? Requires a rather specific set
      # of precomputed references, refer to the SNPeff docs. Personally I've
      # had better luck with bcftools csq, which will be supported here soon.
      snpeff: false


tool_settings:
  # sets a trade-off between compression time and disk usage. If disk is
  # limiting, increase, if CPUS expensive, decrease. Default should be 6.
  ziplevel: 6
  ngm:
    sensitivity: 0.5
  adapterremoval:
    __default__:  # global defaults. modify per qc_type
      adapter_file: "rawdata/adapters.txt"
      minqual: 20
      qualenc: 33
      maxqualval: 45
  kwip:
    kmer_size: 21
    sketch_size: 300000000
  mash:
    kmer_size: 21
    sketch_size: 100000
  varcall:
    # Per-aligner minimum MAPQ thresholds for using a read.
    minmapq:
      bwa: 30  # bwa scores approximately follow a PHRED scale (-10*log10(p))
      ngm: 10  # NGM scores are bonkers, and don't follow a particularly clear scale. In practice ~10 seems appropriate

    # Minimum base quality to count *base* in pileup
    minbq: 15 
    
    # Coverage per region across all samples. This is used to partition the
    # genome in a coverage-aware manner with goleft indexcov.
    # The values below are stupidly low for testing purposes, you should
    # increase the to at least the numbers in the comments.
    region_coverage_threshold:
      mpileup:    5  # 10000
      freebayes:  5  #  1000

    # Stop counting reads after this many. Per variant caller. It depends on the tool as to whether or not this is per-sample or per-file.
    max_depth:
      mpileup:   12000000    # Per file
      freebayes: 12000000    # In total

    # Filters. These are series of command line arguments to pass to bcftools
    # view. These filters apply while multiallelic variants have been
    # decomposed into multiple overlapping variant calls. This allows e.g.
    # allele frequency filters to be performed on a per-allele basis. Please
    # only very lighly filter variants, as variant filtering is a highly task-
    # and question-specific decision, and these variant files should be good
    # for use in any downstream application.
    filters:
      default: >
        -i 'QUAL >= 10 &&
            INFO/DP >= 5 &&
            INFO/AN >= 3'


#######################################################################
#                           Resource Config                           #
#######################################################################
resources:
  # set to true to see all the resources for each rule
  __DEBUG__: false
  __default__:
    # A default set of resources. Jobs not configured individually will inherit these values.
    cores: 1
    disk_mb: 32000
    mem_mb: 1000
    time_min: 120
    localrule: 0  # this must be an integer, so 0 = false & 1 = true
  __max__:
    # A maximum of each resource permitted by your execution environment. Each job's
    # request will be capped at these values.
    # 
    # On cloud environments, use the maximum available resources of your machine type. On
    # HPC clusters, use the size of an individual node. On a local machine, use the size
    # of the machine you run snakemake on.
    cores: 32
    disk_mb: 300000
    mem_mb:  120000
    time_min: 2880
  # What follows overrides both the defaults above and any per-job specification in the 
  # snakemake rules files. The keys of this should be individual rule names as passed to
  # configure_resources() in the snakemake rules files. Run snakemake -np with the
  # __DEBUG__ variable above set to true to see a full list of the defaults.
  example_rule_name:
    cores: 8
    disk_mb: 300000
    mem_mb: 16000
    time_min: 120

# Unlike other code files in Acathophis, this file is placed in the public
# domain, so there are no restrictions on its modification. Specifically, these
# example files are licensed under the Creative Commons Zero licence, as that
# is a more portable concept of "public domain".
